{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and trimming your own model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we detail how to use the learnable masks to trim models that are not listed in the paper or in this repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os \n",
    "import random\n",
    "import typing as tp\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding learnable masks within the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first define a toy network to illustrate the approach. We will define a small convolutional network, however the approach also works for other architectures (as illustrated in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        channels = [16, 32, 64]\n",
    "        out_dim = 10\n",
    "        in_dim = 1\n",
    "        network = []\n",
    "        for c_in, c_out in zip([in_dim]+channels, channels):\n",
    "            network.append(nn.Conv1d(c_in, c_out, kernel_size=5, stride=3))\n",
    "            network.append(nn.BatchNorm1d(c_out))\n",
    "            network.append(nn.ReLU())\n",
    "        network.append(nn.Conv1d(channels[-1], out_dim, kernel_size=3, padding='same'))\n",
    "        self.network = nn.Sequential(*network)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).mean(-1)\n",
    "\n",
    "model = ToyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the architecture of our network and see that it is composed of four 1-d convolutional layers, followed by batch normalization and ReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToyModel(\n",
      "  (network): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(5,), stride=(3,))\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv1d(16, 32, kernel_size=(5,), stride=(3,))\n",
      "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Conv1d(32, 64, kernel_size=(5,), stride=(3,))\n",
      "    (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "    (9): Conv1d(64, 10, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also save the number of parameters of the model for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "untrimmed_param_count = sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets say we want to attach learnable mask modules after each BatchNorm (here, we do not trim the last layer to keep the \"embedding\" dimension unchanged). We first need to define such mask modules, add them to the network, and add hooks to each BatchNorm to post-process their output by applying the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unsqueeze_as(x: torch.Tensor, target: torch.Tensor\n",
    "                  ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Add dimensions to x to match number of dimensions of target\n",
    "    \"\"\"\n",
    "    for _ in range(target.dim()-x.dim()):\n",
    "        x=x.unsqueeze(-1)\n",
    "    return x\n",
    "\n",
    "class MaskModule(nn.Module):\n",
    "    def __init__(self, num_features: int, feature_dim: int=1)->None:\n",
    "        \"\"\"\n",
    "        Learnable mask module. \n",
    "        \n",
    "        args:\n",
    "        num_features (int): Number of features (e.g. convolutional channels, attention heads, ...) of the input\n",
    "        feature_dim (int): Feature dimension of the input\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.feature_dim=feature_dim\n",
    "        self.num_features=num_features\n",
    "        self.mask = nn.Parameter(torch.ones(1, num_features))\n",
    "    \n",
    "    @property\n",
    "    def binary_mask(self):\n",
    "        _bin_mask = torch.round(torch.sigmoid(self.mask)) # Quantize mask values\n",
    "        bin_mask = self.mask + (_bin_mask-self.mask).detach() # Bypass rounding operator during backward\n",
    "        return bin_mask\n",
    "    \n",
    "    @property\n",
    "    def masked_indexes(self):\n",
    "        return torch.where(self.binary_mask==0)[-1].tolist()\n",
    "\n",
    "    @property\n",
    "    def num_masked_features(self):\n",
    "        return int(self.num_features-self.binary_mask.sum().item())\n",
    "    \n",
    "    def set_mask(self, indexes: tp.List[str], keep: bool=False\n",
    "                 )-> None:\n",
    "        if keep:\n",
    "            self.mask.data = torch.zeros_like(self.mask.data)\n",
    "            self.mask.data[:, indexes]=1.\n",
    "        else:\n",
    "            self.mask.data[:, indexes]=0.\n",
    "    \n",
    "    def reset_mask(self):\n",
    "        self.mask.data = torch.ones_like(self.mask.data)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor)->torch.Tensor:\n",
    "        mask = _unsqueeze_as(self.binary_mask, x)\n",
    "        mask = mask.transpose(1, self.feature_dim)\n",
    "        return x*mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, with our architecture, BatchNorm layers will be registered correspond to 'network.1', 'network.4' and 'network.7', hence we append the mask modules to these layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_hook(model, input, output):\n",
    "    \"\"\"\n",
    "    Hook to mask output of a layer\n",
    "    \"\"\"\n",
    "    return model.mask_module(output)\n",
    "\n",
    "for layer in [1, 4, 7]:\n",
    "    num_features = model.network[layer].num_features\n",
    "    model.network[layer].mask_module = MaskModule(num_features=num_features, feature_dim=1)\n",
    "    model.network[layer].register_forward_hook(mask_hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the sparsity inducing loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the paper, the model is guided towards sparsity by adding a loss that minimizes the mean of the parameters values over the masks (small values will then be zeroed during the sigmoid+round operation). Let us define such a loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_masks(masked_model: nn.Module\n",
    "                 ) -> tp.Dict[str, nn.Parameter]:\n",
    "    masks = {}\n",
    "    for n, m in masked_model.named_modules():\n",
    "        if isinstance(m, MaskModule):\n",
    "            masks[n] = m.mask\n",
    "    return masks\n",
    "\n",
    "class SparsityLoss(nn.Module):\n",
    "    def __init__(self, \n",
    "                 target: float = 0.5, \n",
    "                 power: int = 2 \n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.target = target \n",
    "        self.power = power\n",
    "        \n",
    "    def forward(self, masked_model: nn.Module)-> float:\n",
    "        \"\"\"\n",
    "        Note that this loss is applied to the model itself, NOT its output\n",
    "        \"\"\"\n",
    "        masks = gather_masks(masked_model)\n",
    "        if not len(masks.keys()):\n",
    "            return torch.tensor(0.)\n",
    "        loss = 0\n",
    "        for mask in masks.values():\n",
    "            loss = loss+(torch.mean(torch.sigmoid(mask))/len(masks))\n",
    "        loss = (loss-self.target)**(self.power)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's say our objective is classification, and let's denote x the input of the network, y its prediction, and label the ground truth. To add the sparsity loss to this objective, we would do as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss : 2.3212904930114746\n",
      "Sparsity loss : 0.0533880740404129\n",
      "Total loss : 7.660098075866699\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 1, 44100)\n",
    "y = model(x)\n",
    "labels = torch.randint(0, 10, size=(4, ))\n",
    "cross_entropy_fn = nn.CrossEntropyLoss()\n",
    "sparsity_loss_fn = SparsityLoss()\n",
    "sparsity_loss_weight = 100\n",
    "\n",
    "cross_entropy_loss = cross_entropy_fn(y, labels) \n",
    "sparsity_loss = sparsity_loss_fn(model)\n",
    "total_loss = cross_entropy_loss+sparsity_loss_weight*sparsity_loss\n",
    "\n",
    "print(f'Cross-entropy loss : {cross_entropy_loss.item()}')\n",
    "print(f'Sparsity loss : {sparsity_loss.item()}')\n",
    "print(f'Total loss : {total_loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimming the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once, the model is trained, you will need to convert the masks into trimming indexes, and remove the masked units accordingly. Even though many packages allow to this \"automatically\", we demonstrate here how to do it on your own, which should help you avoiding common problems that occur when trimming neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We did not train the model in this notebook, hence the masks are initialized with ones. \n",
    "# Hence, we will just mask random features\n",
    "\n",
    "for n, m in model.named_modules():\n",
    "    if isinstance(m, MaskModule):\n",
    "        m.reset_mask()\n",
    "        masked_channels = random.choices(list(range(m.num_features)), k=10)\n",
    "        m.set_mask(masked_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets feed a random input to our model, this way we can check later that 'true' trimming did not alter the output of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 1, 44100)\n",
    "y_untrimmed = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us say that, for the first mask, the 4-th feature is zeroed (after sigmoid+round). This means that we can remove the 4-th _output_ channel of the preceeding convolution, as well as the 4-th feature of the precedding batch norm. Note that we also have to remove the 4-th _input_ channel of the following convolution as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step : convert the mask into trimming indexes, then remove the mask and its associated hooks\n",
    "\n",
    "for layer in [1, 4, 7]:\n",
    "    trimmed_out_features = model.network[layer].mask_module.masked_indexes\n",
    "    num_features = model.network[layer].mask_module.num_features\n",
    "    kept_out_features = [feat for feat in range(num_features) if feat not in trimmed_out_features] # feature that will not be trimmed\n",
    "    model.network[layer].kept_out_features = kept_out_features\n",
    "    delattr(model.network[layer], 'mask_module') # Delete mask\n",
    "    model.network[layer]._forward_hooks = OrderedDict() # remove hook\n",
    "    \n",
    "# Second step : associate these kept features to the previous and following convolution layers\n",
    "for cur_bn, prev_conv, next_conv in zip([1, 4, 7], [0, 3, 6], [3, 6, 9]):\n",
    "    model.network[prev_conv].kept_out_features = model.network[cur_bn].kept_out_features\n",
    "    model.network[next_conv].kept_in_features = model.network[cur_bn].kept_out_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now remove all units that should not be kept !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, m in model.named_modules():\n",
    "    if hasattr(m, 'kept_in_features'):\n",
    "        m.weight.data = m.weight.data[:, m.kept_in_features]\n",
    "    if hasattr(m, 'kept_out_features'):\n",
    "        m.weight.data = m.weight.data[m.kept_out_features]\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            m.bias.data = m.bias.data[m.kept_out_features]\n",
    "        if hasattr(m, 'running_mean') and m.running_mean is not None:\n",
    "            m.running_mean = m.running_mean[m.kept_out_features]\n",
    "        if hasattr(m, 'running_var') and m.running_var is not None:\n",
    "            m.running_var = m.running_var[m.kept_out_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same output for untrimmed and trimmed models : True\n",
      "Number of parameters : \n",
      "\t - untrimmed : 15146\n",
      "\t - trimmed : 8957\n"
     ]
    }
   ],
   "source": [
    "y_trimmed = model(x)\n",
    "trimmed_param_count = sum([p.numel() for p in model.parameters()])\n",
    "\n",
    "print(f'Same output for untrimmed and trimmed models : {torch.allclose(y_untrimmed, y_trimmed)}')\n",
    "print('Number of parameters : ')\n",
    "print(f'\\t - untrimmed : {untrimmed_param_count}')\n",
    "print(f'\\t - trimmed : {trimmed_param_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and advices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If your goal when using this method is to make models smaller, be careful to attach the mask modules at places that are trimmable  \n",
    "- Sometimes, adding a forward hook doesn't work, so you will have to manually re-write the entire forward method (for examples, see networks/mask_utils.py)\n",
    "- This approach can lead to layer trimming in certain cases (notably for transformers, where bypassing an entire self-attention is sometimes beneficial). In this case, you will also have to modify the forward method of the trimmed layer (see trim/trim_musicfm.py for an example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
